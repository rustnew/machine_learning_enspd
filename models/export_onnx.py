import torch
import torch.nn as nn
import torch.serialization
import json  # <-- IMPORT AJOUT√â ICI
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression

# ========= PATHS =========
PTH_MODEL = "student_model_pro_20260108_223923.pth"
ONNX_MODEL = "student_model.onnx"

# ========= S√âCURIT√â PYTORCH 2.6+ =========
# Autoriser les globals n√©cessaires
torch.serialization.add_safe_globals([
    IsotonicRegression,
    LogisticRegression
])

# ========= LOAD CHECKPOINT =========
print("üîç Chargement du checkpoint...")

# Charger avec weights_only=False
checkpoint = torch.load(PTH_MODEL, map_location="cpu", weights_only=False)

print("‚úÖ Checkpoint charg√©")

# ========= CONFIGURATION =========
# D√©finir les features manuellement pour √©viter d'importer model.py
FEATURE_COLUMNS = [
    'Niveau_etude',
    'Heures_etude_ordinal', 
    'Planning_ordinal',
    'Assiduite_ordinal',
    'Environnement_ordinal',
    'Sommeil_score',
    'Qualite_ordinal'
]

# ========= RECONSTRUCTION DU MOD√àLE =========
# Extraire la configuration
config_dict = checkpoint.get("model_config", {})
input_size = config_dict.get("input_size", 7)
hidden_sizes = tuple(config_dict.get("hidden_sizes", [16, 8]))
dropout_rate = config_dict.get("dropout_rate", 0.2)
normalization = config_dict.get("normalization", "layer")

print(f"üîß Configuration extraite:")
print(f"   ‚Ä¢ Input size: {input_size}")
print(f"   ‚Ä¢ Hidden sizes: {hidden_sizes}")
print(f"   ‚Ä¢ Dropout: {dropout_rate}")
print(f"   ‚Ä¢ Normalization: {normalization}")

# ========= CR√âER LE MOD√àLE SIMPLIFI√â =========
class SimpleMLP(nn.Module):
    """Version simplifi√©e du mod√®le pour l'export ONNX"""
    def __init__(self, input_size=7, hidden_sizes=(16, 8), dropout=0.2, normalization='layer'):
        super(SimpleMLP, self).__init__()
        
        layers = []
        current_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(current_size, hidden_size))
            
            if normalization == 'layer':
                layers.append(nn.LayerNorm(hidden_size))
            elif normalization == 'batch':
                layers.append(nn.BatchNorm1d(hidden_size))
            
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            current_size = hidden_size
        
        layers.append(nn.Linear(current_size, 1))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)

# Cr√©er le mod√®le
model = SimpleMLP(
    input_size=input_size,
    hidden_sizes=hidden_sizes,
    dropout=dropout_rate,
    normalization=normalization
)

# Charger les poids
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()
print("‚úÖ Mod√®le reconstruit")

# ========= V√âRIFICATION =========
print("\nüß™ Test du mod√®le...")
with torch.no_grad():
    # Test avec des valeurs r√©alistes [0, 1]
    test_input = torch.randn(1, input_size) * 0.3 + 0.5  # ~N(0.5, 0.3)
    test_output = model(test_input)
    probability = torch.sigmoid(test_output).item()
    
    # Trouver le seuil optimal
    threshold_info = checkpoint.get('training_results', {}).get('threshold_info', {})
    optimal_threshold = threshold_info.get('optimal_threshold', 0.61)
    
    print(f"   ‚Ä¢ Sortie brute: {test_output.item():.4f}")
    print(f"   ‚Ä¢ Probabilit√©: {probability:.4f}")
    print(f"   ‚Ä¢ Seuil optimal: {optimal_threshold:.3f}")
    print(f"   ‚Ä¢ Pr√©diction: {'R√âUSSITE' if probability >= optimal_threshold else '√âCHEC'}")

# ========= EXPORT ONNX =========
print(f"\nüì§ Export ONNX vers: {ONNX_MODEL}")

try:
    dummy_input = torch.randn(1, input_size)
    
    torch.onnx.export(
        model,
        dummy_input,
        ONNX_MODEL,
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=["features"],
        output_names=["logits"],
        dynamic_axes={
            "features": {0: "batch_size"},
            "logits": {0: "batch_size"}
        },
        verbose=False
    )
    
    print("‚úÖ Export ONNX r√©ussi!")
    
    # ========= V√âRIFICATION FICHIER =========
    import os
    file_size = os.path.getsize(ONNX_MODEL) / 1024 / 1024
    print(f"üìè Taille fichier: {file_size:.2f} MB")
    
    # ========= M√âTADONN√âES POUR RUST =========
    metadata = {
        "model_info": {
            "format": "ONNX",
            "opset_version": 14,
            "input_shape": [1, input_size],
            "features": FEATURE_COLUMNS,
            "export_date": "2026-01-09"
        },
        "inference": {
            "threshold": optimal_threshold,
            "input_range": [0.0, 1.0],
            "output_type": "logits"
        },
        "rust_config": {
            "input_name": "features",
            "output_name": "logits",
            "dtype": "f32",
            "requires_sigmoid": True
        }
    }
    
    with open("model_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
   
    print(f"\n3. Seuil optimal: {optimal_threshold}")
    print("   ‚Ä¢ Calcul: probabilit√© = 1.0 / (1.0 + (-logits).exp())")
    print(f"   ‚Ä¢ D√©cision: probabilit√© >= {optimal_threshold} ‚Üí R√âUSSITE")
    
except Exception as e:
    print(f"‚ùå Erreur export ONNX: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*50)
print("‚úÖ PROCESSUS TERMIN√â!")
print("="*50)