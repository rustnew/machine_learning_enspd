"""
DATA_PIPELINE.PY
Pipeline complet de prÃ©paration des donnÃ©es
"""

import numpy as np
import pandas as pd
from typing import Tuple, Dict, List
import json

class DataPipeline:
    """
    Pipeline complet pour le preprocessing des donnÃ©es Ã©tudiantes
    """
    
    # FEATURES SÃ‰LECTIONNÃ‰ES (selon l'analyse)
    SELECTED_FEATURES = [
        'Niveau_etude',              # ðŸŽ“ Pertinent
        'Heures_etude_ordinal',      # â±ï¸ TRÃˆS PERTINENT
        'Planning_ordinal',          # ðŸ“… TRÃˆS PERTINENT
        'Assiduite_ordinal',         # ðŸ« CRITIQUE
        'Environnement_ordinal',     # ðŸŒ Pertinent
        'Sommeil_score',             # ðŸ˜´ CRITIQUE
        'Qualite_ordinal'            # ðŸ‘¨â€ðŸ« TRÃˆS PERTINENT
    ]
    
    # FEATURES Ã€ EXCLURE (selon l'analyse)
    EXCLUDED_FEATURES = [
        'Mois_Inscription',          # âŒ Faible lien causal
        'Problemes_salles_ordinal',  # âŒ Peu discriminant
        'Effectif_ordinal',          # âš ï¸ Faible Ã  moyen
        'Materiel_ordinal'           # âš ï¸ Faible Ã  moyen
    ]
    
    def __init__(self, data_path: str = 'dataset_strict_7features.csv'):
        """
        Initialise le pipeline de donnÃ©es
        
        Args:
            data_path: Chemin vers le fichier de donnÃ©es
        """
        self.data_path = data_path
        self.scaler_params = None
        self.feature_stats = None
        
    def load_and_validate(self) -> pd.DataFrame:
        """
        Charge et valide les donnÃ©es
        
        Returns:
            DataFrame validÃ©
        """
        print("ðŸ“ Chargement des donnÃ©es...")
        
        try:
            df = pd.read_csv(self.data_path)
            print(f"   âœ… DonnÃ©es chargÃ©es: {df.shape[0]} lignes, {df.shape[1]} colonnes")
        except FileNotFoundError:
            print(f"   âŒ Fichier {self.data_path} non trouvÃ©")
            print("   âš ï¸  GÃ©nÃ©ration de donnÃ©es synthÃ©tiques...")
            df = self._generate_synthetic_data()
        
        # Valider les colonnes requises
        required_columns = self.SELECTED_FEATURES + ['Reussite_binaire']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"   âš ï¸  Colonnes manquantes: {missing_columns}")
            print("   âš ï¸  Tentative de correction...")
            df = self._fix_missing_columns(df, missing_columns)
        
        # VÃ©rifier la distribution des classes
        class_distribution = df['Reussite_binaire'].value_counts(normalize=True)
        print(f"\nðŸ“Š Distribution des classes:")
        for class_val, proportion in class_distribution.items():
            count = (df['Reussite_binaire'] == class_val).sum()
            print(f"   â€¢ Classe {class_val}: {count} Ã©chantillons ({proportion:.1%})")
        
        # VÃ©rifier l'Ã©quilibre
        imbalance_ratio = class_distribution.max() / class_distribution.min()
        if imbalance_ratio > 3:
            print(f"   âš ï¸  Classes dÃ©sÃ©quilibrÃ©es (ratio: {imbalance_ratio:.1f})")
        
        return df
    
    def _generate_synthetic_data(self, n_samples: int = 1000) -> pd.DataFrame:
        """
        GÃ©nÃ¨re des donnÃ©es synthÃ©tiques pour le dÃ©veloppement
        
        Args:
            n_samples: Nombre d'Ã©chantillons Ã  gÃ©nÃ©rer
            
        Returns:
            DataFrame synthÃ©tique
        """
        np.random.seed(42)
        
        # GÃ©nÃ©rer des features rÃ©alistes
        data = {}
        
        # Niveau_etude (0-1, normalisÃ©)
        data['Niveau_etude'] = np.random.beta(2, 2, n_samples)
        
        # Heures d'Ã©tude (0-3, ordinal)
        data['Heures_etude_ordinal'] = np.random.choice([0, 1, 2, 3], n_samples, 
                                                         p=[0.1, 0.2, 0.3, 0.4])
        
        # Planning (0-3, ordinal)
        data['Planning_ordinal'] = np.clip(
            data['Heures_etude_ordinal'] + np.random.randint(-1, 2, n_samples), 0, 3
        )
        
        # AssiduitÃ© (0-3, ordinal)
        data['Assiduite_ordinal'] = np.random.choice([0, 1, 2, 3], n_samples,
                                                     p=[0.05, 0.1, 0.2, 0.65])
        
        # Environnement (0-2, ordinal)
        data['Environnement_ordinal'] = np.random.choice([0, 1, 2], n_samples,
                                                         p=[0.1, 0.3, 0.6])
        
        # Sommeil (0-4, score)
        data['Sommeil_score'] = np.random.choice([0, 1, 2, 3, 4], n_samples,
                                                 p=[0.1, 0.2, 0.3, 0.25, 0.15])
        
        # QualitÃ© enseignement (0-3, ordinal)
        data['Qualite_ordinal'] = np.random.choice([0, 1, 2, 3], n_samples,
                                                   p=[0.05, 0.25, 0.55, 0.15])
        
        # GÃ©nÃ©rer la cible avec logique rÃ©aliste
        success_prob = (
            data['Heures_etude_ordinal'] * 0.15 +
            data['Planning_ordinal'] * 0.12 +
            data['Assiduite_ordinal'] * 0.20 +
            data['Sommeil_score'] * 0.10 +
            data['Qualite_ordinal'] * 0.15 +
            data['Niveau_etude'] * 0.10 +
            data['Environnement_ordinal'] * 0.08 +
            np.random.normal(0, 0.15, n_samples)
        )
        
        data['Reussite_binaire'] = (success_prob > 0.5).astype(int)
        
        df = pd.DataFrame(data)
        print(f"   âœ… DonnÃ©es synthÃ©tiques gÃ©nÃ©rÃ©es: {df.shape}")
        
        return df
    
    def _fix_missing_columns(self, df: pd.DataFrame, missing_columns: List[str]) -> pd.DataFrame:
        """
        Tente de corriger les colonnes manquantes
        
        Args:
            df: DataFrame original
            missing_columns: Liste des colonnes manquantes
            
        Returns:
            DataFrame corrigÃ©
        """
        for col in missing_columns:
            if col == 'Reussite_binaire':
                # GÃ©nÃ©rer une cible synthÃ©tique
                print(f"     â†’ GÃ©nÃ©ration de {col}...")
                df[col] = np.random.choice([0, 1], len(df), p=[0.3, 0.7])
            else:
                # GÃ©nÃ©rer des valeurs alÃ©atoires pour les features
                print(f"     â†’ GÃ©nÃ©ration de {col}...")
                if 'ordinal' in col:
                    df[col] = np.random.randint(0, 4, len(df))
                elif 'score' in col:
                    df[col] = np.random.randint(0, 5, len(df))
                else:
                    df[col] = np.random.random(len(df))
        
        return df
    
    def preprocess(self, df: pd.DataFrame, fit_scaler: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        PrÃ©traite les donnÃ©es: sÃ©lection, normalisation
        
        Args:
            df: DataFrame brut
            fit_scaler: Si True, ajuste le scaler aux donnÃ©es
            
        Returns:
            X: Features prÃ©traitÃ©es (n_samples, n_features)
            y: Labels (n_samples,)
        """
        print("\nðŸ”§ PrÃ©traitement des donnÃ©es...")
        
        # 1. SÃ©lection des features
        print("   1. SÃ©lection des 7 features critiques...")
        X = df[self.SELECTED_FEATURES].values.astype(np.float32)
        y = df['Reussite_binaire'].values.astype(np.float32)
        
        print(f"   â†’ Features: {self.SELECTED_FEATURES}")
        print(f"   â†’ Shape: X={X.shape}, y={y.shape}")
        
        # 2. Calcul des statistiques
        self.feature_stats = {
            'means': X.mean(axis=0),
            'stds': X.std(axis=0),
            'mins': X.min(axis=0),
            'maxs': X.max(axis=0),
            'medians': np.median(X, axis=0)
        }
        
        # 3. Normalisation Min-Max (0-1)
        print("   2. Normalisation Min-Max (0-1)...")
        
        if fit_scaler or self.scaler_params is None:
            X_min = X.min(axis=0, keepdims=True)
            X_max = X.max(axis=0, keepdims=True)
            X_range = X_max - X_min
            
            # Ã‰viter division par zÃ©ro
            X_range[X_range == 0] = 1.0
            
            self.scaler_params = {
                'min': X_min.flatten(),
                'max': X_max.flatten(),
                'range': X_range.flatten()
            }
        
        # Appliquer la normalisation
        X_scaled = (X - self.scaler_params['min']) / self.scaler_params['range']
        
        print(f"   â†’ Normalisation appliquÃ©e")
        print(f"   â†’ Range des features: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]")
        
        return X_scaled, y
    
    def split_data(self, X: np.ndarray, y: np.ndarray, 
                   val_size: float = 0.15, test_size: float = 0.15,
                   random_state: int = 42) -> Tuple:
        """
        Split stratifiÃ© des donnÃ©es
        
        Args:
            X: Features
            y: Labels
            val_size: Proportion pour la validation
            test_size: Proportion pour le test
            random_state: Graine alÃ©atoire
            
        Returns:
            (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        np.random.seed(random_state)
        
        print(f"\nðŸ“Š Split des donnÃ©es...")
        print(f"   Train: {1-val_size-test_size:.0%}, Val: {val_size:.0%}, Test: {test_size:.0%}")
        
        # Indices par classe
        class_0_idx = np.where(y == 0)[0]
        class_1_idx = np.where(y == 1)[0]
        
        # MÃ©langer
        np.random.shuffle(class_0_idx)
        np.random.shuffle(class_1_idx)
        
        # Calculer les tailles
        n_test_0 = int(len(class_0_idx) * test_size)
        n_test_1 = int(len(class_1_idx) * test_size)
        
        n_val_0 = int(len(class_0_idx) * val_size)
        n_val_1 = int(len(class_1_idx) * val_size)
        
        # Indices de test
        test_idx = np.concatenate([
            class_0_idx[:n_test_0],
            class_1_idx[:n_test_1]
        ])
        
        # Indices de validation
        val_idx = np.concatenate([
            class_0_idx[n_test_0:n_test_0 + n_val_0],
            class_1_idx[n_test_1:n_test_1 + n_val_1]
        ])
        
        # Indices d'entraÃ®nement
        train_idx = np.concatenate([
            class_0_idx[n_test_0 + n_val_0:],
            class_1_idx[n_test_1 + n_val_1:]
        ])
        
        # MÃ©langer les indices
        np.random.shuffle(train_idx)
        np.random.shuffle(val_idx)
        np.random.shuffle(test_idx)
        
        # CrÃ©er les splits
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        X_test, y_test = X[test_idx], y[test_idx]
        
        print(f"\nðŸ“ˆ Distribution finale:")
        print(f"   Train:  {X_train.shape[0]:6d} Ã©chantillons "
              f"({X_train.shape[0]/len(X):6.1%})")
        print(f"   Val:    {X_val.shape[0]:6d} Ã©chantillons "
              f"({X_val.shape[0]/len(X):6.1%})")
        print(f"   Test:   {X_test.shape[0]:6d} Ã©chantillons "
              f"({X_test.shape[0]/len(X):6.1%})")
        
        # VÃ©rifier la distribution des classes
        for split_name, split_y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
            n_class_1 = np.sum(split_y == 1)
            proportion = n_class_1 / len(split_y)
            print(f"   {split_name}: {n_class_1:4d} rÃ©ussites ({proportion:.1%})")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def normalize_new_data(self, X_new: np.ndarray) -> np.ndarray:
        """
        Normalise de nouvelles donnÃ©es avec les paramÃ¨tres existants
        
        Args:
            X_new: Nouvelles donnÃ©es (n_samples, n_features)
            
        Returns:
            DonnÃ©es normalisÃ©es
        """
        if self.scaler_params is None:
            raise ValueError("Le scaler doit Ãªtre ajustÃ© d'abord (appeler preprocess avec fit_scaler=True)")
        
        X_scaled = (X_new - self.scaler_params['min']) / self.scaler_params['range']
        return X_scaled
    
    def save_pipeline(self, filepath: str = 'data_pipeline_params.npz'):
        """
        Sauvegarde les paramÃ¨tres du pipeline
        
        Args:
            filepath: Chemin de sauvegarde
        """
        pipeline_data = {
            'scaler_params': self.scaler_params,
            'feature_stats': self.feature_stats,
            'selected_features': self.SELECTED_FEATURES,
            'excluded_features': self.EXCLUDED_FEATURES
        }
        
        np.savez(filepath, **pipeline_data)
        print(f"âœ… Pipeline sauvegardÃ©: {filepath}")
    
    @classmethod
    def load_pipeline(cls, filepath: str = 'data_pipeline_params.npz'):
        """
        Charge un pipeline sauvegardÃ©
        
        Args:
            filepath: Chemin du fichier
            
        Returns:
            Instance de DataPipeline avec paramÃ¨tres chargÃ©s
        """
        pipeline_data = np.load(filepath, allow_pickle=True)
        
        # CrÃ©er une instance
        pipeline = cls()
        
        # Charger les paramÃ¨tres
        pipeline.scaler_params = pipeline_data['scaler_params'].item()
        pipeline.feature_stats = pipeline_data['feature_stats'].item()
        
        print(f"âœ… Pipeline chargÃ©: {filepath}")
        return pipeline
    
    def analyze_features(self, df: pd.DataFrame) -> Dict:
        """
        Analyse statistique des features
        
        Args:
            df: DataFrame avec les donnÃ©es
            
        Returns:
            Dictionnaire d'analyse
        """
        print("\nðŸ” Analyse statistique des features...")
        
        analysis = {}
        
        for feature in self.SELECTED_FEATURES:
            if feature in df.columns:
                feature_data = df[feature]
                
                # Statistiques
                stats = {
                    'mean': float(feature_data.mean()),
                    'std': float(feature_data.std()),
                    'min': float(feature_data.min()),
                    'max': float(feature_data.max()),
                    'median': float(feature_data.median()),
                    'skewness': float(feature_data.skew()),
                    'correlation_with_target': float(df[[feature, 'Reussite_binaire']].corr().iloc[0, 1])
                }
                
                # CatÃ©goriser l'importance
                corr_abs = abs(stats['correlation_with_target'])
                if corr_abs > 0.3:
                    importance = 'ðŸ”¥ TRÃˆS FORTE'
                elif corr_abs > 0.2:
                    importance = 'âœ… FORTE'
                elif corr_abs > 0.1:
                    importance = 'âš ï¸ MODÃ‰RÃ‰E'
                else:
                    importance = 'âŒ FAIBLE'
                
                stats['importance'] = importance
                analysis[feature] = stats
                
                # Affichage
                print(f"   â€¢ {feature:25s}: corr={stats['correlation_with_target']:+.3f} - {importance}")
        
        return analysis
    
    def create_batches(self, X: np.ndarray, y: np.ndarray, 
                      batch_size: int = 32, shuffle: bool = True) -> list:
        """
        CrÃ©e des batches pour l'entraÃ®nement
        
        Args:
            X: Features
            y: Labels
            batch_size: Taille des batches
            shuffle: Si True, mÃ©lange les donnÃ©es
            
        Returns:
            Liste de tuples (X_batch, y_batch)
        """
        n_samples = X.shape[0]
        
        if shuffle:
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
        else:
            X_shuffled = X
            y_shuffled = y
        
        batches = []
        for i in range(0, n_samples, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            if len(X_batch) == batch_size or i + batch_size >= n_samples:
                batches.append((X_batch, y_batch))
        
        return batches